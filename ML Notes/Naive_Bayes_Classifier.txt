1) These are statistical classifiers, which use Bayes Theorem to calculate probabilities. 

2) Let X be a data sample and H be the hypotheses that X belongs to a class. 
  P(X) - evidence, P(H|X) - posteriori probablity, P(H) - prior probability, P(X|H) - likelihood

  posteriori = likelihood * prior / evidence
  
3) Computationally costly. Works well with large datasets.

4) Naive bayes is a naive approach since it treats all word orders as same. In case of document classification, it ignores all the grammar, punctutation.
It just consider language as the bag of words. It has high bias and low variance. 

Pros - 
1. Easy and fast
2. Better performance in independence and categorical variables

Cons - 
1. Zero frequency (Use Laplace estimation, add 1)
2. Known as bad estimator
3. Independent predictor assumption

Applications - Credit scoring, Medical data classification, real time prediction (eager learner), multiclass prediction, text classification, spam filtering, 
sentiment analysis, recommdation system

Gaussian NB - Features follow normal distribution
Multinomial NB - discrete counts
Bernoulli NB - Features have binary values

-> If continuous features don't have normal distribution, make it normal.
-> For zero frequency problem, apply laplace estimation
-> Remove correlated features
-> Focus on preprocessing and feature selection
